{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c7cbd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "https://www.kaggle.com/datasets/ramjasmaurya/poem-classification-nlp?select=Poem_classification+-+train_data.csv\n",
    "Contains poems and their genre. Affection,Environment,Music and Death. Separated by training and test data sets\n",
    "\n",
    "## Motivation\n",
    "Super interested in NLP and making models understand human language. This time I wanted to use neural nets for my models to see if I can accurately predict poem genre. I will be using LSTM, Bidirectional LSTM and CNN. Regular RNN is not used often in the real world due to the vanishing gradient problem where as LSTM is preferred. CNN is usually used for image classification but I wanted to set it up for nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405be530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, losses, optimizers,callbacks\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense,LSTM, Bidirectional,Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63528b1d",
   "metadata": {},
   "source": [
    "## Loading dataset and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d960a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>In the thick brushthey spend the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>Storms are generous.                       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>—After Ana Mendieta Did you carry around the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Aja Sherrard at 20The portent may itself ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                               Poem\n",
       "0  Music                                                NaN\n",
       "1  Music                In the thick brushthey spend the...\n",
       "2  Music     Storms are generous.                       ...\n",
       "3  Music   —After Ana Mendieta Did you carry around the ...\n",
       "4  Music   for Aja Sherrard at 20The portent may itself ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Poem_classification - train_data.csv\")\n",
    "test_df = pd.read_csv(\"Poem_classification - test_data.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8939d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre    0\n",
       "Poem     4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b95aaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre    0\n",
       "Poem     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6777c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d36d3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre    0\n",
       "Poem     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968a5ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>In the thick brushthey spend the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>Storms are generous.                       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>—After Ana Mendieta Did you carry around the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Aja Sherrard at 20The portent may itself ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Bob Marley, Bavaria, November 1980 Here i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                               Poem\n",
       "0  Music                In the thick brushthey spend the...\n",
       "1  Music     Storms are generous.                       ...\n",
       "2  Music   —After Ana Mendieta Did you carry around the ...\n",
       "3  Music   for Aja Sherrard at 20The portent may itself ...\n",
       "4  Music   for Bob Marley, Bavaria, November 1980 Here i..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b63da",
   "metadata": {},
   "source": [
    "## Converting the catagorical data to numerical labels \n",
    "had issues before where the neural net would not accept the labels as words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa45bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "train_df['label'] = labelencoder.fit_transform(train_df['Genre'])\n",
    "test_df['label'] = labelencoder.fit_transform(test_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5273327f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>In the thick brushthey spend the...</td>\n",
       "      <td>3</td>\n",
       "      <td>in the thick brushthey spend the hottest part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>Storms are generous.                       ...</td>\n",
       "      <td>3</td>\n",
       "      <td>storm are generous something so easy to surren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>—After Ana Mendieta Did you carry around the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>after ana mendieta did you carry around the ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Aja Sherrard at 20The portent may itself ...</td>\n",
       "      <td>3</td>\n",
       "      <td>for aja sherrard at 20the portent may itself b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Bob Marley, Bavaria, November 1980 Here i...</td>\n",
       "      <td>3</td>\n",
       "      <td>for bob marley bavaria november 1980 here is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                               Poem  label  \\\n",
       "0  Music                In the thick brushthey spend the...      3   \n",
       "1  Music     Storms are generous.                       ...      3   \n",
       "2  Music   —After Ana Mendieta Did you carry around the ...      3   \n",
       "3  Music   for Aja Sherrard at 20The portent may itself ...      3   \n",
       "4  Music   for Bob Marley, Bavaria, November 1980 Here i...      3   \n",
       "\n",
       "                                          clean_poem  \n",
       "0  in the thick brushthey spend the hottest part ...  \n",
       "1  storm are generous something so easy to surren...  \n",
       "2  after ana mendieta did you carry around the ma...  \n",
       "3  for aja sherrard at 20the portent may itself b...  \n",
       "4  for bob marley bavaria november 1980 here is t...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d6d7cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>A woman walks by the bench I’m sitting onwith ...</td>\n",
       "      <td>3</td>\n",
       "      <td>a woman walk by the bench im sitting onwith he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>Because I am a boy, the untouchability of beau...</td>\n",
       "      <td>3</td>\n",
       "      <td>because i am a boy the untouchability of beaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>Because today we did not leave this world,We n...</td>\n",
       "      <td>3</td>\n",
       "      <td>because today we did not leave this worldwe no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>Big Bend has been here, been here. Shouldn’t i...</td>\n",
       "      <td>3</td>\n",
       "      <td>big bend ha been here been here shouldnt it ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>I put shells there, along the lip of the road....</td>\n",
       "      <td>3</td>\n",
       "      <td>i put shell there along the lip of the roadbiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                               Poem  label  \\\n",
       "0  Music  A woman walks by the bench I’m sitting onwith ...      3   \n",
       "1  Music  Because I am a boy, the untouchability of beau...      3   \n",
       "2  Music  Because today we did not leave this world,We n...      3   \n",
       "3  Music  Big Bend has been here, been here. Shouldn’t i...      3   \n",
       "4  Music  I put shells there, along the lip of the road....      3   \n",
       "\n",
       "                                          clean_poem  \n",
       "0  a woman walk by the bench im sitting onwith he...  \n",
       "1  because i am a boy the untouchability of beaut...  \n",
       "2  because today we did not leave this worldwe no...  \n",
       "3  big bend ha been here been here shouldnt it ha...  \n",
       "4  i put shell there along the lip of the roadbiv...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b5bc003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Music          238\n",
       "Death          231\n",
       "Environment    227\n",
       "Affection      141\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e95b8762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    238\n",
       "1    231\n",
       "2    227\n",
       "0    141\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ae22a",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6d433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. function that makes all text lowercase.\n",
    "def make_lowercase(test_string):\n",
    "    return test_string.lower()\n",
    "\n",
    "# 2. function that removes all punctuation. \n",
    "def remove_punc(test_string):\n",
    "    test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "    return test_string\n",
    "\n",
    "# 3. function that removes all stopwords.\n",
    "def remove_stopwords(test_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(test_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords. Stopwords was imported from nltk.corpus\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "\n",
    "# 4. function to break words into their lemm words\n",
    "def lem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        lemmed_word = lemmatizer.lemmatize(word) #from nltk.stem import PorterStemmer\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(lemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84adb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(a_string):\n",
    "    a_string = make_lowercase(a_string)\n",
    "    a_string = remove_punc(a_string)\n",
    "    a_string = lem_words(a_string)\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273deecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"clean_poem\"] = train_df[\"Poem\"].apply(text_processing_pipeline)\n",
    "test_df[\"clean_poem\"] = test_df[\"Poem\"].apply(text_processing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e68b179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poem</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>In the thick brushthey spend the...</td>\n",
       "      <td>3</td>\n",
       "      <td>in the thick brushthey spend the hottest part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>Storms are generous.                       ...</td>\n",
       "      <td>3</td>\n",
       "      <td>storm are generous something so easy to surren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>—After Ana Mendieta Did you carry around the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>after ana mendieta did you carry around the ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Aja Sherrard at 20The portent may itself ...</td>\n",
       "      <td>3</td>\n",
       "      <td>for aja sherrard at 20the portent may itself b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>for Bob Marley, Bavaria, November 1980 Here i...</td>\n",
       "      <td>3</td>\n",
       "      <td>for bob marley bavaria november 1980 here is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                               Poem  label  \\\n",
       "0  Music                In the thick brushthey spend the...      3   \n",
       "1  Music     Storms are generous.                       ...      3   \n",
       "2  Music   —After Ana Mendieta Did you carry around the ...      3   \n",
       "3  Music   for Aja Sherrard at 20The portent may itself ...      3   \n",
       "4  Music   for Bob Marley, Bavaria, November 1980 Here i...      3   \n",
       "\n",
       "                                          clean_poem  \n",
       "0  in the thick brushthey spend the hottest part ...  \n",
       "1  storm are generous something so easy to surren...  \n",
       "2  after ana mendieta did you carry around the ma...  \n",
       "3  for aja sherrard at 20the portent may itself b...  \n",
       "4  for bob marley bavaria november 1980 here is t...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911f217",
   "metadata": {},
   "source": [
    "## Vectorize X_train\n",
    "basically converting each poem to a vector of size 500.\n",
    "doing this so that the nueral nets can accept them as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854493b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 500)\n"
     ]
    }
   ],
   "source": [
    "# Limiting our tokenizers vocab size to 10000\n",
    "max_words = 10000\n",
    " \n",
    "    \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(train_df[\"clean_poem\"])\n",
    "\n",
    "\n",
    "# Create the sequences for each sentence, basically turning each word into its index position\n",
    "sequences = tokenizer.texts_to_sequences(train_df[\"clean_poem\"])\n",
    "\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "\n",
    "# # Limiting our sequencer to only include 500 words\n",
    "max_length = 500\n",
    "\n",
    "\n",
    "# # Convert the sequences to all be the same length of 500\n",
    "X_train = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be157fde",
   "metadata": {},
   "source": [
    "## Vectorize X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6493850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 500)\n"
     ]
    }
   ],
   "source": [
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(train_df[\"clean_poem\"])\n",
    "\n",
    "# Create the sequences for each sentence, basically turning each word into its index position\n",
    "sequences = tokenizer.texts_to_sequences(test_df[\"clean_poem\"])\n",
    "\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "\n",
    "# # Limiting our sequencer to only include 500 words\n",
    "max_length = 500\n",
    "\n",
    "\n",
    "# # Convert the sequences to all be the same length of 500\n",
    "X_test = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63340494",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train_df['label'])\n",
    "y_test = to_categorical(test_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac6218",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f819ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 500, 50)           30200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 500, 50)           20200     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,070,804\n",
      "Trainable params: 1,070,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This creates the Neural Network\n",
    "model = Sequential() \n",
    "\n",
    "# This embedding layer basically will automatically create the word2vec vectors based on your text data.\n",
    "model.add( Embedding(max_words, 100, input_length=max_length) ) \n",
    "model.add(LSTM(50, return_sequences=True,dropout =0.2))\n",
    "model.add(LSTM(50,return_sequences=True,dropout =0.2))\n",
    "model.add(LSTM(50,dropout =0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d0d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - 14s 289ms/step - loss: 1.3161 - accuracy: 0.3303 - val_loss: 2.2999 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 9s 278ms/step - loss: 1.2859 - accuracy: 0.3214 - val_loss: 2.2105 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 9s 271ms/step - loss: 1.2822 - accuracy: 0.3513 - val_loss: 2.8827 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 9s 267ms/step - loss: 1.2886 - accuracy: 0.3244 - val_loss: 2.2593 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 1.2827 - accuracy: 0.3498 - val_loss: 2.4348 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 1.2847 - accuracy: 0.3423 - val_loss: 2.6886 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 9s 265ms/step - loss: 1.2822 - accuracy: 0.3453 - val_loss: 2.1212 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 9s 265ms/step - loss: 1.2830 - accuracy: 0.3468 - val_loss: 2.4404 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 1.2797 - accuracy: 0.3558 - val_loss: 2.4586 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 1.2798 - accuracy: 0.3393 - val_loss: 2.5023 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 1.2801 - accuracy: 0.3558 - val_loss: 2.3878 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 1.2816 - accuracy: 0.3333 - val_loss: 2.5120 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 9s 265ms/step - loss: 1.2818 - accuracy: 0.3572 - val_loss: 2.3934 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 1.2810 - accuracy: 0.3558 - val_loss: 2.5587 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 1.2803 - accuracy: 0.3587 - val_loss: 2.3969 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 9s 262ms/step - loss: 1.2806 - accuracy: 0.3378 - val_loss: 2.4944 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 1.2798 - accuracy: 0.3602 - val_loss: 2.4503 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 9s 265ms/step - loss: 1.2821 - accuracy: 0.3438 - val_loss: 2.3575 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 1.2811 - accuracy: 0.3543 - val_loss: 2.4737 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 9s 279ms/step - loss: 1.2806 - accuracy: 0.3318 - val_loss: 2.3463 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 1.2813 - accuracy: 0.3333 - val_loss: 2.3748 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 9s 267ms/step - loss: 1.2797 - accuracy: 0.3558 - val_loss: 2.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 1.2834 - accuracy: 0.3558 - val_loss: 2.3583 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 9s 270ms/step - loss: 1.2815 - accuracy: 0.3393 - val_loss: 2.4965 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 1.2790 - accuracy: 0.3572 - val_loss: 2.4381 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 9s 267ms/step - loss: 1.2786 - accuracy: 0.3558 - val_loss: 2.4110 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 1.2809 - accuracy: 0.3393 - val_loss: 2.3928 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 9s 270ms/step - loss: 1.2792 - accuracy: 0.3558 - val_loss: 2.4352 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 1.2791 - accuracy: 0.3572 - val_loss: 2.4370 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 1.2792 - accuracy: 0.3378 - val_loss: 2.4897 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2, \n",
    "                 epochs=30, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fceab579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked LSTM: 0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print('Test accuracy with stacked LSTM:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5810df3",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3953c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              49664     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 370,180\n",
      "Trainable params: 370,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, 32, input_length=max_length))\n",
    "model.add(Bidirectional(layers.LSTM(64, dropout=0.2)))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7d51f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - 6s 113ms/step - loss: 1.3248 - accuracy: 0.3318 - val_loss: 2.0274 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 4s 104ms/step - loss: 1.2807 - accuracy: 0.3543 - val_loss: 2.2408 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 4s 112ms/step - loss: 1.2242 - accuracy: 0.3842 - val_loss: 2.3651 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 4s 106ms/step - loss: 1.0061 - accuracy: 0.5889 - val_loss: 2.5173 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 4s 103ms/step - loss: 0.7821 - accuracy: 0.6786 - val_loss: 2.5985 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.6420 - accuracy: 0.7250 - val_loss: 2.2271 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 3s 102ms/step - loss: 0.4611 - accuracy: 0.8251 - val_loss: 2.3137 - val_accuracy: 0.0179\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 3s 102ms/step - loss: 0.3166 - accuracy: 0.9058 - val_loss: 2.1579 - val_accuracy: 0.2024\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.2660 - accuracy: 0.9118 - val_loss: 1.6009 - val_accuracy: 0.4167\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 3s 102ms/step - loss: 0.2426 - accuracy: 0.9253 - val_loss: 2.6664 - val_accuracy: 0.1131\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 3s 103ms/step - loss: 0.1698 - accuracy: 0.9238 - val_loss: 2.8466 - val_accuracy: 0.1488\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 4s 105ms/step - loss: 0.1840 - accuracy: 0.9238 - val_loss: 2.7295 - val_accuracy: 0.0774\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 3s 102ms/step - loss: 0.1656 - accuracy: 0.9223 - val_loss: 2.8872 - val_accuracy: 0.1488\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.1472 - accuracy: 0.9357 - val_loss: 2.9430 - val_accuracy: 0.1429\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.1517 - accuracy: 0.9193 - val_loss: 3.1221 - val_accuracy: 0.1190\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.1415 - accuracy: 0.9357 - val_loss: 2.5837 - val_accuracy: 0.1607\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.1319 - accuracy: 0.9283 - val_loss: 2.9026 - val_accuracy: 0.1369\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 3s 103ms/step - loss: 0.1388 - accuracy: 0.9283 - val_loss: 3.2453 - val_accuracy: 0.0536\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 3s 100ms/step - loss: 0.1282 - accuracy: 0.9297 - val_loss: 3.2527 - val_accuracy: 0.0774\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 4s 104ms/step - loss: 0.1209 - accuracy: 0.9312 - val_loss: 3.1409 - val_accuracy: 0.1310\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 4s 112ms/step - loss: 0.1198 - accuracy: 0.9253 - val_loss: 3.1165 - val_accuracy: 0.1071\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 4s 108ms/step - loss: 0.1186 - accuracy: 0.9223 - val_loss: 3.2597 - val_accuracy: 0.1071\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 4s 106ms/step - loss: 0.1146 - accuracy: 0.9283 - val_loss: 3.2325 - val_accuracy: 0.1131\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 4s 105ms/step - loss: 0.1089 - accuracy: 0.9312 - val_loss: 3.3052 - val_accuracy: 0.1131\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 4s 103ms/step - loss: 0.1146 - accuracy: 0.9268 - val_loss: 2.9500 - val_accuracy: 0.1607\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 3s 103ms/step - loss: 0.1148 - accuracy: 0.9327 - val_loss: 3.3253 - val_accuracy: 0.1071\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 3s 103ms/step - loss: 0.1060 - accuracy: 0.9253 - val_loss: 3.4106 - val_accuracy: 0.0833\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 3s 102ms/step - loss: 0.1146 - accuracy: 0.9268 - val_loss: 3.4129 - val_accuracy: 0.0893\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 4s 113ms/step - loss: 0.1022 - accuracy: 0.9357 - val_loss: 3.5399 - val_accuracy: 0.0714\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 3s 101ms/step - loss: 0.1080 - accuracy: 0.9268 - val_loss: 3.5275 - val_accuracy: 0.0833\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2, \n",
    "                 epochs=30, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe8c0320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Bidirectional LSTM: 0.2266666740179062\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print('Test accuracy with Bidirectional LSTM:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0516a7",
   "metadata": {},
   "source": [
    "## Cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87a71c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 494, 32)           7200      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 98, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 92, 32)            7200      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 334,532\n",
      "Trainable params: 334,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_words, 32, input_length=max_length))\n",
    "\n",
    "model.add(Conv1D(32, 7, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(32, 7, activation='relu'))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a282a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - 1s 12ms/step - loss: 1.3125 - accuracy: 0.3498 - val_loss: 2.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 1.2532 - accuracy: 0.3812 - val_loss: 2.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 1.2057 - accuracy: 0.6278 - val_loss: 2.3902 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 1.1383 - accuracy: 0.6577 - val_loss: 2.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 1.0252 - accuracy: 0.6517 - val_loss: 2.6119 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.8154 - accuracy: 0.6682 - val_loss: 2.7484 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.5530 - accuracy: 0.8371 - val_loss: 3.1127 - val_accuracy: 0.0060\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.3497 - accuracy: 0.8834 - val_loss: 2.6546 - val_accuracy: 0.0417\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.2640 - accuracy: 0.9253 - val_loss: 2.4812 - val_accuracy: 0.0595\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1948 - accuracy: 0.9208 - val_loss: 2.6104 - val_accuracy: 0.0774\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1865 - accuracy: 0.9133 - val_loss: 3.0206 - val_accuracy: 0.0655\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1476 - accuracy: 0.9417 - val_loss: 3.0868 - val_accuracy: 0.0774\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1618 - accuracy: 0.9163 - val_loss: 3.2425 - val_accuracy: 0.0774\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1449 - accuracy: 0.9178 - val_loss: 3.6186 - val_accuracy: 0.0595\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1459 - accuracy: 0.9253 - val_loss: 3.4745 - val_accuracy: 0.0595\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1321 - accuracy: 0.9193 - val_loss: 3.5121 - val_accuracy: 0.0774\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1311 - accuracy: 0.9223 - val_loss: 3.7518 - val_accuracy: 0.0655\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1368 - accuracy: 0.9208 - val_loss: 3.7103 - val_accuracy: 0.0655\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1275 - accuracy: 0.9133 - val_loss: 3.7898 - val_accuracy: 0.0655\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1178 - accuracy: 0.9312 - val_loss: 4.1152 - val_accuracy: 0.0655\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1262 - accuracy: 0.9208 - val_loss: 4.0407 - val_accuracy: 0.0655\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1265 - accuracy: 0.9253 - val_loss: 4.2906 - val_accuracy: 0.0536\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1226 - accuracy: 0.9223 - val_loss: 3.8384 - val_accuracy: 0.0774\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9268 - val_loss: 4.3075 - val_accuracy: 0.0536\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1233 - accuracy: 0.9208 - val_loss: 4.2521 - val_accuracy: 0.0655\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1164 - accuracy: 0.9208 - val_loss: 4.2245 - val_accuracy: 0.0655\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1130 - accuracy: 0.9312 - val_loss: 4.2975 - val_accuracy: 0.0595\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1229 - accuracy: 0.9208 - val_loss: 4.4032 - val_accuracy: 0.0536\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1129 - accuracy: 0.9223 - val_loss: 4.3260 - val_accuracy: 0.0655\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1189 - accuracy: 0.9193 - val_loss: 4.3577 - val_accuracy: 0.0595\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2, \n",
    "                 epochs=30, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c66ab5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with CNN: 0.36000001430511475\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aff7fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 774ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2120746 , 0.33572647, 0.08293388, 0.36926502]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = 'Roses are red Voilets are blue but I know I do not love you.'\n",
    "sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "model.predict(padded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1352b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I got poor results for these nueral net. Stacked LSTM was underfitting with accuracy and validation accuracy being low. Low test accuracy as well. Bidirectional LSTM did better than LSTM on test accuracy but overfitted during training. Maybe I should add more layer and dropout. Cnn was overfitting as well but perform better than bidirectional LSTM. \n",
    "\n",
    "What was weird was that it took only a few seconds for each epoch for all neural nets. I am not sure what can be done about that since I would prefer each epoch to take its time. And I would have prefered more data, since neural nets are data hungry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
