{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Exam\n",
    "\n",
    "This is the final exam.\n",
    "\n",
    "It is due at 12:30 in the afternoon on 12/20/22.\n",
    "\n",
    "I have been very happy that there has been almost no cheating in this course so far.\n",
    "\n",
    "Please do not collaborate or try to find this code already written online.\n",
    "\n",
    "Please email me or post on Piazza if you have any questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prologue\n",
    "\n",
    "The main part of the exam is about implementing a simple neural network as described in the notes,`NN_fall_22.ipynb` (you should have this under `Handouts`).\n",
    "\n",
    "That notebook will be a good guide for what follows.  I cannot overemphasize how much you need that file -- if you cannot find it please let me know.\n",
    "\n",
    "Please have a look at the code in the cell below, copied here for convenience and as a persistent record:\n",
    "\n",
    "```python\n",
    "W_1 = np.array([[0.1,0.2],[0.3,0.4]])\n",
    "W_2 = np.array([[0.2],[1],[-3]])\n",
    "W_3 = np.array([[1],[2]])\n",
    "\n",
    "w = [W_1,W_2,W_3]\n",
    "x = np.array([1,2])\n",
    "Xs,S=forward_prop(x,w)\n",
    "print(\"Xs = \",Xs)\n",
    "print(\"S = \",S)\n",
    "yn=1\n",
    "Deltas=backprop(Xs,yn,S,w)\n",
    "print(\"deltas = \",Deltas)\n",
    "print(\"The gradient is...\")\n",
    "NNgrad(Xs,Deltas)\n",
    "```\n",
    "\n",
    "The code makes reference to three functions that have not been written.\n",
    "\n",
    "1) `forwardprop(x,y)`\n",
    "2) `backprop(Xs,yn,S,w)`\n",
    "3) `NNgrad(Xs,Deltas)`\n",
    "\n",
    "As you might expect, these functions implement forward propagation, backpropagation, and the computation of the neural network gradient.  \n",
    "\n",
    "All of these steps are done in the worked example at the end of the notes.  Also the pseudocode for each of these functions is provided in the notes.\n",
    "\n",
    "If the functions are implemented correctly, the output should be something like the following.\n",
    "\n",
    "```python\n",
    "Xs =  {0: array([1, 2]), 1: array([1.        , 0.60436778, 0.76159416]), 2: array([ 1.        , -0.90154565]), 3: array([ 1.        , -0.66576144])}\n",
    "S =  {1: array([0.7, 1. ]), 2: array([-1.48041469]), 3: array([-0.80309131])}\n",
    "deltas =  {3: array([-1.85486437]), 2: array([-0.69451848]), 1: array([-0.44083838,  0.87503983])}\n",
    "The gradient is...\n",
    "[array([[-0.44083838,  0.87503983],\n",
    "        [-0.88167675,  1.75007965]]),\n",
    " array([[-0.69451848],\n",
    "        [-0.41974459],\n",
    "        [-0.52894122]]),\n",
    " array([[-1.85486437],\n",
    "        [ 1.67224491]])]\n",
    "```\n",
    "\n",
    "I say \"something like\" because in my implementation I made some design choices that you do not necessarily have to follow.\n",
    "\n",
    "1. I used dictionaries rather than arrays for `Xs`,`S` and `Deltas`. \n",
    "2. I include a bias in the output layer of `Xs` even though it is never used, because this makes the code a little simpler.  It does not strictly need to be there.\n",
    "\n",
    "In terms of mathematical notation used in the notes,\n",
    "\n",
    "`Xs[i]` $= {\\bf x}^{(i)}$ \n",
    "\n",
    "`S[i]` $= {\\bf s}^{(i)}$\n",
    "\n",
    "`Deltas[i]` $={\\bf \\delta}^{(i)}$ \n",
    "\n",
    "`w` $= [W^{(1)},W^{(2)},\\ldots,W^{(L)}]$\n",
    "\n",
    "`x` $= {\\bf{x}^{(0)}}$\n",
    "\n",
    "`yn` $= y^{(n)}$ (here the $n$ is the index in the sample set -- nothing to do with neural net layers).\n",
    "\n",
    "*Hint*  All of this code is basically done at the end of the notes on neural nets.  It is just not expressed in terms of functions, and it only works for $L=3$. You just need to adapt that code to work for other $L$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.2\u001b[39m],[\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.4\u001b[39m]])\n\u001b[0;32m      2\u001b[0m W_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.2\u001b[39m],[\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]])\n\u001b[0;32m      3\u001b[0m W_3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m2\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "W_1 = np.array([[0.1,0.2],[0.3,0.4]])\n",
    "W_2 = np.array([[0.2],[1],[-3]])\n",
    "W_3 = np.array([[1],[2]])\n",
    "\n",
    "w = [W_1,W_2,W_3]\n",
    "x = np.array([1,2])\n",
    "Xs,S=forward_prop(x,w)\n",
    "print(\"Xs = \",Xs)\n",
    "print(\"S = \",S)\n",
    "yn=1\n",
    "Deltas=backprop(Xs,yn,S,w)\n",
    "print(\"deltas = \",Deltas)\n",
    "print(\"The gradient is...\")\n",
    "NNgrad(Xs,Deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Implement `forward_prop(x,w)` as described in the prologue.\n",
    "\n",
    "You should use the data provided as a test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "theta = lambda z:np.tanh(z)\n",
    "def forward_prop(x,w):\n",
    "    t_x = {0:x}\n",
    "    t_s = {}\n",
    "    count = 1\n",
    "    for num in range(len(w)):\n",
    "        t_s[num+1] = w[num].T@t_x[num]\n",
    "        if num == len(w) -1:\n",
    "            t_x[num+1] = np.hstack(theta(t_s[num+1]))\n",
    "        else:\n",
    "            t_x[num+1] = np.hstack(([1],theta(t_s[num+1])))\n",
    "    return t_x,t_s\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.array([[0.1,0.2],[0.3,0.4]])\n",
    "W_2 = np.array([[0.2],[1],[-3]])\n",
    "W_3 = np.array([[1],[2]])\n",
    "\n",
    "w = [W_1,W_2,W_3]\n",
    "x = np.array([1,2])\n",
    "Xs,S=forward_prop(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([0.7, 1. ]), 2: array([-1.48041469]), 3: array([-0.80309131])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([1, 2]),\n",
       " 1: array([1.        , 0.60436778, 0.76159416]),\n",
       " 2: array([ 1.        , -0.90154565]),\n",
       " 3: array([-0.66576144])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Implement `backprop(Xs,yn,S,w)` as described in the prologue.\n",
    "\n",
    "You should use the data provided as a test case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaPrime = lambda z: 1-np.tanh(z)**2\n",
    "def backprop(Xs,yn,S,w):\n",
    "    d = {}\n",
    "    for num in reversed(range(len(w))):\n",
    "        if num + 1 == len(w):\n",
    "            d[len(w)] = yn*(Xs[num+1] -1)*thetaPrime(S[num+1])\n",
    "            #d = {len(w) : yn*(Xs[num+1] -1)*thetaPrime(S[num+1])}\n",
    "        else:\n",
    "            d[num+1] = thetaPrime(S[num+1])*(w[num+1]@d[num+2])[1:]\n",
    "            #d = {num+1: thetaPrime(S[num+1])*(w[num+1]@d[num+2])[1:]}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.array([[0.1,0.2],[0.3,0.4]])\n",
    "W_2 = np.array([[0.2],[1],[-3]])\n",
    "W_3 = np.array([[1],[2]])\n",
    "\n",
    "w = [W_1,W_2,W_3]\n",
    "x = np.array([1,2])\n",
    "Xs,S=forward_prop(x,w)\n",
    "Deltas=backprop(Xs,2,S,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: array([-1.85486437]),\n",
       " 2: array([-0.69451848]),\n",
       " 1: array([-0.44083838,  0.87503983])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Implement `NNgrad(Xs,Deltas)` as discussed in the prologue.\n",
    "\n",
    "You should use the data provided as a test case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNgrad(Xs,Deltas):\n",
    "    e = {}\n",
    "    for num in range(len(Deltas)):\n",
    "        e[num+1] = np.outer(Xs[num],Deltas[num+1].T)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[-0.44083838,  0.87503983],\n",
       "        [-0.88167675,  1.75007965]]),\n",
       " 2: array([[-0.69451848],\n",
       "        [-0.41974459],\n",
       "        [-0.52894122]]),\n",
       " 3: array([[-1.85486437],\n",
       "        [ 1.67224491]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNgrad(Xs,Deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude\n",
    "\n",
    "If you have done the first 3 problems then you have basically implemented a neural network with `tanh` activation.\n",
    "\n",
    "Congratulations!  That is not easy.\n",
    "\n",
    "In this cell I am pasting in some other functions that would let you actually use your network on real data.\n",
    "\n",
    "This is just for entertainment/education and has nothing to do with the exam.\n",
    "\n",
    "The code basically uses `fit` as you would in the sklearn API.\n",
    "\n",
    "```python\n",
    "\n",
    "def predict(X,w):\n",
    "    Xbias = np.c_[np.ones(X.shape[0]),X] #bias\n",
    "    yhat = np.ones(Xbias.shape[0])\n",
    "    for i,x in enumerate(Xbias):\n",
    "        Xs,S = forward_prop(x,w,theta)\n",
    "        yhat[i] = (Xs[len(w)][-1]>0)*2-1\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def grad_add(w,wgrad,alpha=0,eta=0.1):\n",
    "    for i in range(len(w)):\n",
    "        w[i] -= eta*wgrad[i]-alpha*w[i]/w[0].shape[0]\n",
    "\n",
    "def error(X,y,w):\n",
    "    yhat = np.ones(X.shape[0])\n",
    "    for i,x in enumerate(X):\n",
    "        Xs,S = forward_prop(x,w)\n",
    "        yhat[i] = Xs[len(w)][-1]\n",
    "    return ((y - yhat)**2).sum()/len(y)\n",
    "\n",
    "def stoch_grad_desc(X,y,arch=[10,10,1],eta=0.1,alpha=0.1,max_iter=300):\n",
    "    _arch = [X.shape[1]-1]+arch\n",
    "    w = [np.random.randn(_arch[i]+1,_arch[i+1])/10 for i in range(len(_arch)-1)]\n",
    "    errors=[]\n",
    "    for epochs in range(max_iter):\n",
    "        p = np.random.permutation(X.shape[0])\n",
    "        Xp,yp = X[p],y[p]\n",
    "        for x,yn in zip(Xp,yp):\n",
    "            Xs,S= forward_prop(x,w)\n",
    "            Deltas=backprop(Xs,yn,S,w)\n",
    "            wgrad = NNgrad(Xs,Deltas)\n",
    "            grad_add(w,wgrad,alpha,eta)\n",
    "            #errors.append(error(X,y,w))  ## this takes too long to always run\n",
    "    return w,errors\n",
    "\n",
    "def fit(X,y,arch=[10,10,1],eta=0.1,alpha=0.1,max_iter=300):\n",
    "    Xbias = np.c_[np.ones(X.shape[0]),X] #bias\n",
    "    return stoch_grad_desc(Xbias,y,arch,eta,alpha,max_iter)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "Use TensorFlow to do either classification or regression on some dataset.\n",
    "\n",
    "The easiest way to use TF is on Colab.\n",
    "\n",
    "I highly suggest you use Colab if you do not already have TF on your system.\n",
    "\n",
    "Please include the dataset you use in this directory so that I can run your code.\n",
    "\n",
    "And please include your code (obviously :) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
