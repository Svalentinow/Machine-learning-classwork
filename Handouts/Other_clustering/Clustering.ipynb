{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "\n",
    "The vast majority of data is unlabeled.\n",
    "\n",
    "Labeling data is costly.\n",
    "\n",
    "Clustering offers the promise of detecting classes automatically, with no nead for human intervention.\n",
    "\n",
    "It also has a variety of applications, such as \n",
    "\n",
    "* Data analysis\n",
    "* Customer segmentation\n",
    "* Recommender systems\n",
    "* Search engines\n",
    "* Image segmentation.\n",
    "\n",
    "Let's expand on some of these...\n",
    "\n",
    "\n",
    "#### Data analysis\n",
    "\n",
    "The existence of clusters is itself an interesting feature of data. \n",
    "\n",
    "A data analysis problem might be simplified by using clustering to split the data into types and then attacking each cluster separately. \n",
    "\n",
    "#### Dimensionality reduction\n",
    "\n",
    "For each cluster $C$ and data point $\\bar{x}$, the *affinity* of $\\bar{x}$ for $C$ is a mathematical measure of how well $\\bar{x}$ fits into $C$.\n",
    "\n",
    "If the data has clusters $C_1,C_2,\\ldots,C_k$, we can do a feature transformation\n",
    "\n",
    "$$\\bar{z} = \\Phi(\\bar{x})$$\n",
    "\n",
    "Where $\\bar{z} \\in \\mathbb{R}^k$, and $\\bar{z}_i = Af(\\bar{x},C_i)$.\n",
    "\n",
    "In most contexts this will reduce the dimensionality of the problem in a way different from the linear methods we discussed last time. \n",
    "\n",
    "It has some similarity to the RBF feature transform, with clusters playing the role of landmarks. \n",
    "\n",
    "In fact these two methods can be combined.\n",
    "\n",
    "#### Anomaly Detection\n",
    "\n",
    "We can identify anomalous instances $\\bar{x}$ as those that have low affinity for all clusters. \n",
    "\n",
    "#### Semi-supervised learning\n",
    "\n",
    "Imagine we have a dataset with some but not all of the data labeled.\n",
    "\n",
    "Clustering offers a way to \"interpolate\" and assign missing labels to data. \n",
    "\n",
    "\n",
    "#### Search engines\n",
    "\n",
    "Clusters of images might all be pictures of the same object.\n",
    "\n",
    "Reverse image search might return hits from the cluster to which a submitted image belongs. \n",
    "\n",
    "#### Segmenting an image\n",
    "\n",
    "Image segmentation is the process of automatically breaking a picture up into the \"things\" that occur in the photograph. \n",
    "\n",
    "Here is one clustering based method.\n",
    "\n",
    "1. Cluster pixels according to color.\n",
    "2. Replace each pixel with the mean color of its cluster.\n",
    "3. The image will now consist of contiguous pieces of same-color sections which may correspond to objects. \n",
    "\n",
    "\n",
    "![img](imseg.png)\n",
    "\n",
    "#### Hierarchical clustering\n",
    "\n",
    "Some algorithms seek clusters within clusters.\n",
    "\n",
    "Cluster 1: Men\n",
    "\n",
    "          Subclusters: mustache, bald, beard, etc.\n",
    "          \n",
    "Cluster 2: Cars\n",
    "\n",
    "          Subclusters: Minivans, coupes, etc. \n",
    "          \n",
    "\n",
    "We do this in biology with cladistics.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hierarchical_clustering#/media/File:Iris_dendrogram.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Various algorithms\n",
    "\n",
    "Here you can see some visualizations comparing various clustering algorithms:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "Discovered by Stuart Lloyd, Bell Labs, 1957.  \n",
    "\n",
    "Proprietary until 1982 (though independently rediscovered in 1965).\n",
    "\n",
    "Well described here:\n",
    "\n",
    "http://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf\n",
    "\n",
    "![img](kmc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![img](voronoi.png)\n",
    "\n",
    "---\n",
    "\n",
    "K-means clustering is guaranteed to converge (stabilize) though perhaps not with the ideal centroids.\n",
    "\n",
    "There is luck involved in the initialization of the centroids.\n",
    "\n",
    "By **inertia** we mean the mean squared distance between each instance and the closest centroid.\n",
    "\n",
    "Inertia is a measure of the \"goodness of fit\" for a given set of centroids.\n",
    "\n",
    "### Initialization strategies\n",
    "\n",
    "A naive way to control against the uncertainty stemming from centroid initialization is to do it many times and remember only the best initial configuration.\n",
    "\n",
    "That is, the initialization that leads to the smallest inertia. \n",
    "\n",
    "### K-means++\n",
    "\n",
    "In 2006 an improved way to pick the initial centroids was proposed by David Arthur and Sergei Vassilvitskii.\n",
    "\n",
    "The intuition is to select initial centroids that are distant from one another. \n",
    "\n",
    "The authors showed that the smarter initialization step drastically reduces the number of times the algorithm needs to be run to find the optimal solution. \n",
    "\n",
    "\n",
    "Steps in K-means++ initialization:\n",
    "\n",
    "1. Take one centroid $\\bar{c}^{(1)}$ chosen randomly from the dataset. \n",
    "2. Create a new centroid $\\bar{c}^{(i)}$ by choosing an instance $\\bar{x}^{(i)}$ from the dataset at random with probability $$P(\\bar{x}^{(i)})=\\frac{D(\\bar{x}^{(i)})^2}{\\sum_{j=1}^N D(\\bar{x}^{(j)})^2},$$ where $D(\\bar{x}^{(i)})$ is the distance between $\\bar{x}^{(i)}$ and the closest centroid that has already been chosen.  \n",
    "3. Repeat the previous step until all $k$ centroids have been chosen. \n",
    "\n",
    "This policy ensures that instances farther away from the already chosen centroids are much more likely to be selected as centroids. \n",
    "\n",
    "### Minibatch\n",
    "\n",
    "Minibatch K-means clustering uses a small random selection of data at each step rather than the whole dataset.\n",
    "\n",
    "This is good for huge datasets. \n",
    "\n",
    "\n",
    "![img](minibatch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing K, the number of clusters\n",
    "\n",
    "When you can't obviously see how many clusters there are, you need a way to choose K intelligently.\n",
    "\n",
    "\n",
    "![img](choosek.png)\n",
    "\n",
    "\n",
    "\n",
    "We can't use inertia, because as $K \\rightarrow \\infty$, inertia $\\rightarrow 0$. \n",
    "\n",
    "Every point is zero distance from itself.\n",
    "\n",
    "#### Silhouette score\n",
    "\n",
    "For a given K and accompanying centroids, an instance's silhouette score is\n",
    "\n",
    "$$S(\\bar{x}) = \\frac{b-a}{\\max(a,b)}$$\n",
    "\n",
    "where  \n",
    "\n",
    "$a$ = the mean distance to other instances in the same cluster\n",
    "\n",
    "$b$ = the mean distance to points in the nearest cluster\n",
    "\n",
    "When $S(\\bar{x}) \\approx +1$ the instance $\\bar{x}$ is well inside its own cluster.\n",
    "\n",
    "When $S(\\bar{x}) \\approx -1$ the instance may have been assigned to the wrong cluster.\n",
    "\n",
    "When $S(\\bar{x}) \\approx 0$, the instance is close to a cluster boundary.\n",
    "\n",
    "One way to pick K is to select\n",
    "\n",
    "$$\\text{argmax}_{K} \\frac{1}{N} \\sum S(\\bar{x})$$\n",
    "\n",
    "This can be computationally expensive to compute.\n",
    "\n",
    "![img](silhouette.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Limitations of K means\n",
    " \n",
    " The algorithm is fast and scalable.\n",
    " \n",
    " But...\n",
    " \n",
    " * we must run several times to avoid suboptimal solutions\n",
    " * we need to pick K which can be hard\n",
    " * K-means does not perform well when clusters have varying sizes or non-spherical shapes.\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBScan\n",
    "\n",
    "This algorithm defines **clusters** as contiguous regions of high density.\n",
    "\n",
    "Here is how it works.\n",
    "\n",
    "1.  For each instance, the algorithm counts how many instances are located within a small distance $\\epsilon$ from it.  This region is the instance's $\\epsilon$-neighborhood.\n",
    "1.  If an instance has at least `min_samples` instances in its $\\epsilon$-neighborhood (including itself), then it is considered a *core instance*. In other words, core instances are located in dense regions.\n",
    "1.  All instances in the neighborhood of a core instance belong to the same cluster.  This neighborhood may include other core instances.  A long sequence of neighboring core instances form a single cluster. \n",
    "1.  Any instance which is not a core instance and does not have one in its neighborhood is considered an anomaly.\n",
    "\n",
    "![img](dbscan.png)\n",
    "\n",
    "\n",
    "We can cluster to assign labels and then apply a classifier to obtain a decision boundary.  Anomalies are labeled with +. \n",
    "\n",
    "\n",
    "![img](knndbscan.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixtures\n",
    "\n",
    "This method assumes that all the data were generated from a mixture of several Gaussian distributions whose parameters are unknown.\n",
    "\n",
    "All instances generated by the same distribution form a cluster.\n",
    "\n",
    "Each cluster can have a different ellipsoidal shape, size, density and orientation. \n",
    "\n",
    "![img](https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_covariances_001.png)\n",
    "\n",
    "How \"free\" the distributions are depends on assumptions about the covariance matrix of the joint normal distributions.\n",
    "\n",
    "![img](https://i.stack.imgur.com/FINUY.png)\n",
    "\n",
    "There are several GMM variants.\n",
    "\n",
    "This presentation is for the simplest.\n",
    "\n",
    "You must know K in advance.\n",
    "\n",
    "The dataset $X$ is assumed to have been generated through the following probabilistic process.\n",
    "\n",
    "1.  For each instance, a cluster is picked randomly from among K clusters.  The probability of choosing the $j$ th cluster is defined by the cluster's weight $\\phi^{(j)}$. The index of the cluster chosen for the $i$ th instance is $z^{(i)}$. \n",
    "1.  If $z^{(i)}=j$, meaning the $i$ th instance has been assigned to the $j$ th cluster, the location of $\\bar{x}^{(i)}$ is sampled randomly from the Gaussian distribution with mean $\\bar{\\mu}^{(j)}$ and covariance matrix $\\Sigma^{(j)}$. $$\\bar{x}^{(i)} \\sim \\mathcal{N}(\\bar{\\mu}^{(j)},\\Sigma^{(j)})$$\n",
    "\n",
    "### Assigning to clusters\n",
    "\n",
    "Given $X$ we want to estimate the weights $\\phi$ and the distribution parameters $\\bar{\\mu}^{(1)},\\bar{\\mu}^{(2)},\\ldots,\\bar{\\mu}^{(K)},$ and $\\Sigma^{(1)},\\Sigma^{(2)},\\ldots,\\Sigma^{(K)}$\n",
    "\n",
    "How are these inferred from the data $X$?\n",
    "\n",
    "Using the *Expectation-Maximization* algorithm.\n",
    "\n",
    "This is a sort of generalization of the K-means clustering algorithm.\n",
    "\n",
    "During the update step we not only pick new centeroids $\\bar{\\mu}$ but update the $\\phi$'s and $\\Sigma$ s as well. \n",
    "\n",
    "\n",
    "### Anomaly detection\n",
    "\n",
    "We can define **anomalies** as points that end up in an area that has less than a fixed density threshold. \n",
    "\n",
    "![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimages.deepai.org%2Fglossary-terms%2Fd073e8d10e054f4ca2982d273fcaa0c6%2Fgaussian_mixture.png&f=1&nofb=1)\n",
    "\n",
    "![img](gmm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
