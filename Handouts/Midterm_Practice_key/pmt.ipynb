{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Midterm\n",
    "\n",
    "This is to give you an idea of what the midterm might be like.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Consider the King-Rook vs King dataset.\n",
    "\n",
    "[https://archive.ics.uci.edu/ml/datasets/Chess+%28King-Rook+vs.+King%29](https://archive.ics.uci.edu/ml/datasets/Chess+%28King-Rook+vs.+King%29)\n",
    "\n",
    "\n",
    "I will load the data in the next cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>d</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22485</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>2</td>\n",
       "      <td>fourteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>2</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21304</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "      <td>fourteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "      <td>g</td>\n",
       "      <td>6</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5         6\n",
       "5954   d  3  a  5  f  2     eight\n",
       "22485  b  2  f  7  e  2  fourteen\n",
       "1960   d  1  h  3  h  2      draw\n",
       "21304  b  1  a  1  f  3  fourteen\n",
       "1576   c  3  g  6  g  5      draw"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont.\n",
    "\n",
    "What is the shape of this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28056, 7)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont.\n",
    "\n",
    "Replace the letters in columns 0,2, and 4 with numerical codes. \n",
    "\n",
    "Update the dataframe in place.\n",
    "\n",
    "*Note: This step is not necessary if you are using `sklearn` but for now let's assume we are going to use our own homemade code or some less user friendly library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(list(\"abcdefgh\"),[0,1,2,3,4,5,6,7],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont.\n",
    "\n",
    "Show us value counts for each of the categories in the last column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fourteen    4553\n",
       "thirteen    4194\n",
       "twelve      3597\n",
       "eleven      2854\n",
       "draw        2796\n",
       "fifteen     2166\n",
       "ten         1985\n",
       "nine        1712\n",
       "eight       1433\n",
       "seven        683\n",
       "six          592\n",
       "five         471\n",
       "sixteen      390\n",
       "two          246\n",
       "four         198\n",
       "three         81\n",
       "one           78\n",
       "zero          27\n",
       "Name: 6, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[6].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont.\n",
    "\n",
    "Let $X$ be the columns 0,1,2,3,4 and 5, and make $y$ the last column of the dataframe.\n",
    "\n",
    "For now leave $X$ and $y$ as `pandas` objects (don't convert to `numpy`).\n",
    "\n",
    "Print the first 5 lines of $X$ and $y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5\n",
      "0  0  1  1  3  2  2\n",
      "1  0  1  2  1  2  2\n",
      "2  0  1  2  1  3  1\n",
      "3  0  1  2  1  3  2\n",
      "4  0  1  2  2  2  1\n",
      "0    draw\n",
      "1    draw\n",
      "2    draw\n",
      "3    draw\n",
      "4    draw\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:,0:5]\n",
    "y = df.loc[:,6]\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont\n",
    "\n",
    "Right now `y` has many classes but we want to do binary classification.\n",
    "\n",
    "Use the existing `y` to make a new `y_f` that has a 1 in each position where `y` has either \"thirteen\" or \"fourteen\" and -1 at every other index. \n",
    "\n",
    "Then show us the value counts for the new `y_f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    19309\n",
       " 1     8747\n",
       "Name: 6, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_f = ((y==\"thirteen\")|(y==\"fourteen\"))*2-1\n",
    "y_f.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont\n",
    "\n",
    "Scale the data and do a test/train split (using `sklearn` or with just `numpy`).\n",
    "\n",
    "Print the shapes of the training set, training targets, test set, and test targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21042, 6), (7014, 6), (21042,), (7014,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y_f)\n",
    "\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train)\n",
    "X_train_scl = sclr.transform(X_train)\n",
    "X_test_scl = sclr.transform(X_test)\n",
    "\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 cont\n",
    "\n",
    "Now let's try some classifiers on the data and see what kind of accuracy we can get. \n",
    "\n",
    "Here are some classifiers you can import:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "\n",
    "The SVC is the support vector machine (in classifier mode). We have not talked about the RandomForestClassifier so just consider it a \"black box\". \n",
    "\n",
    "For each classifier, get the best performance you can on the training set. \n",
    "\n",
    "For the linear classifiers you may need to adjust `C` or other hyperparameters. It is also possible to adjust the Random Forest hyperparameters, but since we don't know what they mean yet, let's leave them alone for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654120330767038"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Perceptron()\n",
    "clf = RandomForestClassifier()\n",
    "clf = LogisticRegression()\n",
    "clf = SVC(C=100)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Whew! Now, rather than look deeply at one dataset, let's talk about some generic ML issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Jamie is trying to fit a dataset with a model. Her training set accuracy is 95% but her test set accuracy is only 80%. \n",
    "\n",
    "What is possibly happening here, and how can Jamie fix it?  In your answer use each of these words at least once:\n",
    "\n",
    "`underfitting, overfitting, regularization, bias, variance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor's answer:\n",
    "\n",
    "Jamie's model is overfitting. \n",
    "\n",
    "With enough data, for any given model, training performance and test performance eventually coincide. One solution for Jamie is simply to get more data. In this case the model would eventually achieve a level of performance at some point between 95% and 80%.\n",
    "\n",
    "But let's assume she can't do that because it's somehow too expensive to get more data.  Jamie needs to decide if her error is mostly coming from \"bias\" or from \"variance\".  \n",
    "\n",
    "When training error is much lower than test error (as in this case), the model is too complex for the amount of data that is available. This is a \"high variance\" scenario.  The model needs to be simpler.\n",
    "\n",
    "The fact that training error is very low suggests that bias is not the main issue.\n",
    "\n",
    "Jamie should try a simpler model (eg a linear model) and/or she should use some amount of regularization.\n",
    "\n",
    "When she \"turns up\" the regularization a little bit, she will see her training accuracy come down, but her test accuracy go up.  With some experimentation she will find the level of regularization that gives the best performance on the test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Bobby has tons of data.  His dataset is huge.  But the performance of his model is not great.  It only achieves 60% accuracy, both on the training set and the test set. \n",
    "\n",
    "What might be going on here, and how can Bobby fix it? In your answer use each of these words at least once:\n",
    "\n",
    "\n",
    "`underfitting, overfitting, regularization, bias, variance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor's answer\n",
    "\n",
    "It is possible that Bobby is simply trying to solve an inherently tough problem and 60% accuracy is the best that the state of the art can do with his dataset. \n",
    "\n",
    "But it is consistent with Bobby's situation that he might have an underfitting problem. \n",
    "\n",
    "In this case Bobby is trying a model that is too simple.  With lots of data he can afford to use a complex model with lots of parameters. \n",
    "\n",
    "The first thing to do is to turn regularization down or off.  This should make both training and test performance improve. \n",
    "\n",
    "If Bobby is using an inherently simple model such as a linear model, he might need to add more features, perhaps by using PolynomialFeatures. \n",
    "\n",
    "He could also try switching to an inherently complex model such as a deep neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Suppose we are using the MSE error function $J({\\bf w}) = \\frac{2}{N}\\sum_{i=1}^N (y^{(i)} - {\\bf w}^T{\\bf x}^{(i)})^2$, as in Adaline. \n",
    "\n",
    "We can implement weight decay regularization by using the augmented error \n",
    "\n",
    "$J_{aug}({\\bf w}) = \\frac{1}{2N}\\sum_{i=1}^N (y^{(i)} - {\\bf w}^T{\\bf x}^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{i=0}^d {\\bf w}_i^2$.\n",
    "\n",
    "What is $\\nabla_{\\bf w} J_{aug}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$\\nabla_{\\bf w} J_{aug} = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - {\\bf w}^T{\\bf x}^{(i)}){\\bf x}^{(i)} + {\\lambda} {\\bf w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Implement Adaline with weight decay regularization.  Allow the user to adjust $\\lambda$.\n",
    "\n",
    "It happens that `lambda` is a Python keyword, so (if you prefer) you can call the regularization hyperparameter `alpha` instead. \n",
    "\n",
    "Test your code on the Chess dataset (you will need to add a bias column to `X_train` and `X_test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Jgrad_aug(X,y,w,alpha):\n",
    "    yhat = X@w\n",
    "    return (y-yhat)@X/X.shape[0] +alpha*w\n",
    "\n",
    "def fit_adaline(X,y,alpha=1,num_epochs=500,eta=0.1):\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    for _ in range(num_epochs):\n",
    "        w += eta*Jgrad_aug(X,y,w,alpha)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7192757342457942"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scl_b  = np.c_[np.ones(X_train_scl.shape[0]),X_train_scl]\n",
    "X_test_scl_b  = np.c_[np.ones(X_test_scl.shape[0]),X_test_scl]\n",
    "\n",
    "w = fit_adaline(X_train_scl_b,y_train,eta=0.1,num_epochs=4000,alpha=0.25)\n",
    "\n",
    "yhat = (X_test_scl_b@w>0)*2-1\n",
    "(yhat == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What are some signs that the learning rate $\\eta$ might be too big?\n",
    "\n",
    "What are signs that $\\eta$ might be too small?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution\n",
    "\n",
    "If $\\eta$ is too big, then optimization will fail to converge. For example `w` will be all `nan`s. \n",
    "\n",
    "If $\\eta$ is too small, then optimization will take too long, because the steps toward the optimum are too small. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What are the advantages and disadvantages of stochastic gradient descent over gradient descent?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution\n",
    "\n",
    "Generally speaking, SGD will reach the zone around the optimum value of ${\\bf w}$ more quickly than gradient descent. \n",
    "\n",
    "However, once it gets there, it tends to wander around and might never stabilize at the actual optimum.\n",
    "\n",
    "That is why there are schemes that use SGD with an automatically decreasing learning rate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Consider the MSE function $J$.\n",
    "\n",
    "$J({\\bf w}) = \\frac{2}{N}\\sum_{i=1}^N (y^{(i)} - {\\bf w}^T{\\bf x}^{(i)})^2$\n",
    "\n",
    "Define ${\\bf e}^{(i)} = 2(y^{(i)} - {\\bf w}^T{\\bf x}^{(i)})^2$ to be the error that accrues due to the $i^{th}$ sample. \n",
    "\n",
    "Then $J({\\bf w}) = \\frac{1}{N}\\sum_{i=1}^N {\\bf e}^{(i)}$.\n",
    "\n",
    "In other words, the MSE is the average of the ${\\bf e}^{(i)}$. \n",
    "\n",
    "Let $\\pi$ be a probability distribution that selects $i \\in \\{1,2,\\ldots,N\\}$ uniformly at random.\n",
    "\n",
    "Show that $\\mathbb{E_{i \\sim \\pi}}[\\nabla_{\\bf w} {\\bf e}^{(i)}] = \\nabla_{\\bf w}J$.\n",
    "\n",
    "This proves that in stochastic gradient descent, we move in the right direction \"on average\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution\n",
    "\n",
    "$\\mathbb{E_{i \\sim \\pi}}[\\nabla_{\\bf w} {\\bf e}^{(i)}] = \\frac{1}{N}\\sum_{i=1}^N \\nabla_{\\bf w} {\\bf e}^{(i)} = \\nabla_{\\bf w}  \\frac{1}{N}\\sum_{i=1}^N {\\bf e}^{(i)} = \\nabla_{\\bf w}J({\\bf w})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
